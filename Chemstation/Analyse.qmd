---
title: "Analyse of LCMS data"
author: "Konrad Kr√§mer"
format: 
  html:
    code-fold: true
---

```{r}
#| echo: false
ca <- function(a, b) {
  if ((a < 0) || (b < 0)) {
    return(FALSE)
  }
  (1 - a / b) < 0.1 # 0.1
}

find_start_point <- function(env, start, max_mz) {
  for (i in start:length(env$mz)) {
    if (ca(env$mz[i], max_mz)) {
      return(i)
    }
  }
}

find_end_point <- function(env, start, min_mz) {
  if (length(start) == 0) {
    return()
  }
  prev <- env$mz[start]
  prev_idx <- start
  for (i in start:length(env$mz)) {
    if (ca(min_mz, env$mz[i])) {
      return(i)
    }
    if (prev < env$mz[i]) {
        return(i - 1)
    } else {
      prev <- env$mz[i]
      prev_idx <- i
    }
  }
  return(prev_idx)
}

read <- function(file_path, type, endian, signed = TRUE) {
  type_size <- NULL
  if (type == "uint16") {
    type_size <- 2
    type <- "integer"
  } else if (type == "int") {
    type_size <- 4
  } else if (type == "double") {
    type_size <- 4 # ???
  } else if (type == "raw") {
    type_size <- 1
  } else {
    stop("Invalid type")
  }
  con <- file(file_path, "rb")
  file_size <- file.info(file_path)$size
  data_start <- 752 # Assuming data starts at 0x02f0 (752 in decimal)
  data_size <- file_size - data_start
  num_elements <- data_size / type_size
  seek(con, where = data_start, origin = "start")
  data <- readBin(
    con, type,
    size = type_size,
    n = num_elements,
    endian = endian,
    signed = signed
  )
  close(con)
  return(data)
}

# TODO: does not work 100% correctly?
detect_areas <- function(env, start, min_mz, max_mz) {
  starts <- find_start_point(env, start, max_mz)
  ends <- find_end_point(env, starts, min_mz)
  if ((length(starts) == 0) || (length(ends) == 0)) {
    return()
  }
  if (is.null(starts) || is.null(ends)) {
    return()
  }
  if (ends >= length(env$data)) {
    return()
  }
  if (abs(ends - length(env$mz)) <= 1) {
    return()
  }
  start <- ends + 1
  if ((ends - starts) > 3) {
    env$areas <- append(env$areas, list(c(starts, ends)))
  }
  detect_areas(env, start, min_mz, max_mz)
}

remove_garbage <- function(mz, intensities, min_mz, max_mz) {
  remove_idx <- which(mz < min_mz * 0.9)
  remove_idx <- c(remove_idx, which(mz > max_mz * 1.1))
  mz <- mz[-remove_idx]
  intensities <- intensities[-remove_idx]
  remove_idx <- which(intensities < 0)
  if (length(remove_idx) > 0) {
    mz <- mz[-remove_idx]
    intensities <- intensities[-remove_idx]
  }
  return(list(mz, intensities))
}

normalise <- function(intensities) {
  max_intensity <- max(intensities)
  intensities / max_intensity
}

split_data <- function(data, env, min_mz, max_mz) {
  data <- data / 20
  n <- length(data)
  mz_raw <- data[seq(1, n, 2)]
  intensities_raw <- data[seq(2, n, 2)] * 20
  l <- remove_garbage(mz_raw, intensities_raw, min_mz, max_mz)
  mz <- l[[1]]
  intensities <- l[[2]]
  # intensities <- normalise(intensities)
  env$mz <- mz
  env$intensities <- intensities

  # Split the data into areas
  detect_areas(env, 1, min_mz, max_mz)
  mz_splitted <- lapply(env$areas, function(x) mz[x[1]:x[2]])
  intensities_splitted <- lapply(env$areas, function(x) intensities[x[1]:x[2]])

  # Add the rest as one dataset, even though it does not make sense
  length_end <- length(mz_raw) - sum(sapply(env$mz, function(x) length(x)))
  mz_end <- tail(mz_raw, length_end) |> head(500)
  intensities_end <- tail(intensities_raw, length_end) |> head(500)
  mz_splitted <- c(mz_splitted, list(mz_end))
  intensities_splitted <- c(intensities_splitted, list(intensities_end))

  env$mz <- mz_splitted
  env$intensities <- intensities_splitted
}

gaussian_kernel <- function(size, sigma) {
  x <- seq(-size / 2, size / 2, by = 1)
  kernel <- exp(-x^2 / (2 * sigma^2))
  return(kernel / sum(kernel))
}
apply_gaussian_smoothing <- function(env, idx, window_size = 1, sigma = 1) {
  peak_region_intensities <- env$intensities[[idx]]
  kernel <- gaussian_kernel(window_size, sigma)
  mean_intensity <- mean(peak_region_intensities)
  padded_data <- c(
    rep(mean_intensity, window_size),
    peak_region_intensities, rep(mean_intensity, window_size)
  )
  smoothed_intensities <- convolve(padded_data, kernel, type = "filter")
  smoothed_intensities <- spline(
    x = seq_along(smoothed_intensities), y = smoothed_intensities,
    xout = seq_along(peak_region_intensities)
  )$y
  env$smoothed_intensities[[idx]] <- smoothed_intensities
  if (idx == length(env$mz)) {
    return()
  }
  apply_gaussian_smoothing(env, idx + 1, window_size, sigma)
}
```

## Hypothesis:

- uint16s are used for m/z values and intensities.
- They alternate, so the pattern would be:
- 1st uint16: m/z,
- 2nd uint16: intensity,
- 3rd uint16: m/z,
- 4th uint16: intensity, and so on.


## TIC:

```{r}
# knitr::include_graphics("/home/konrad/Documents/GitHub/LCMS/Agilent-ChemStation/TIC.png")
knitr::include_graphics("TIC_SVS1025F1.png")
```

```{r}
#| warning: false
file_path <- "./SVS-1025F1.D/MSD1.MS" # 100 - 1500
min_mz <- 100
max_mz <- 1500
data <- read(file_path, "uint16", "big", signed = FALSE)
# Signed is FALSE this is important. Thereby, the 8*10^5 is the same TIC dimension as in vendor software
env <- new.env()
env$data <- data
split_data(data, env, min_mz, max_mz)
apply_gaussian_smoothing(env, 1, 1)
# Calc TIC
smoothed_tic<- sapply(seq_len(length(env$mz)), function(i) {
  sum(env$smoothed_intensities[[i]])
})
plot(smoothed_tic, type = "l")
tic <- sapply(seq_len(length(env$mz)), function(i) {
  sum(env$intensities[[i]])
})
plot(tic, type = "l")
```

## Patterns:

```{r}
data <- read(file_path, "uint16", "big", signed = FALSE)
even <- data[
  (seq_along(data) %% 2) == 0
]
even <- even[even > 150] # This is the threshold???
odd <- data[
  (seq_along(data) %% 2) != 0
] / 20
xwindow <- c(1E04, 4E04)
plot(odd, cex = 0.01, col = "darkred", xlim = xwindow)
plot(even, cex = 0.1, xlim = xwindow)
even_splitted <- do.call(c, env$intensities)
plot(even_splitted, cex = 0.1, xlim = xwindow)
```


## Entire splitted data:

- address problem that the last part of the data is removed in preprocessing

```{r}
data <- read(file_path, "uint16", "big", signed = FALSE)
even <- data[
  (seq_along(data) %% 2) == 0
]
even <- even[even > 150] # This is the threshold???
odd <- data[
  (seq_along(data) %% 2) != 0
] / 20
plot(even, cex = 0.1)
even_splitted <- do.call(c, env$intensities)
plot(even_splitted, cex = 0.1)
```


## Compare mz spectra:

- Even with gaussian smoothing and baseline correction, the relative peak intensities are not the same.

```{r}
knitr::include_graphics("mz_spectra.png")
```


```{r}

baseline_correction <- function(mz, intensities, span = 50) {
  baseline <- stats::lowess(mz,
    intensities, f = span / length(intensities))$y
  corrected <- intensities - baseline
  corrected[corrected < 0] <- 0
  return(corrected)
}

# smoothed
max_peak <- which.max(smoothed_tic)
intensity_smoothed <- env$smoothed_intensities[[max_peak]]
intensity_smoothed <- baseline_correction(env$mz[[max_peak]], intensity_smoothed, 500)
intensity_smoothed <- (intensity_smoothed / max(intensity_smoothed)) * 100
mz_max_peak <- env$mz[[max_peak]]
indices <- which(intensity_smoothed > 40)
plot(
  mz_max_peak, intensity_smoothed, type = "l",
  ylim = c(-1, max(intensity_smoothed) * 1.2),
  xlim = c(100, 800)
)
labels <- round(mz_max_peak[indices], 2)
text(
  mz_max_peak[indices], intensity_smoothed[indices],
  labels, pos = 3, cex = 0.5
)

# Not smoothed
max_peak <- which.max(smoothed_tic)
intensity <- env$intensities[[max_peak]]
intensity <- baseline_correction(env$mz[[max_peak]],intensity )
intensity <- (intensity / max(intensity )) * 100
mz_max_peak <- env$mz[[max_peak]]
indices <- which(intensity > 40)
plot(
  mz_max_peak,intensity , type = "l",
  ylim = c(-1, max(intensity ) * 1.2),
  xlim = c(100, 800)
)
labels <- round(mz_max_peak[indices], 2)
text(
  mz_max_peak[indices],intensity [indices],
  labels, pos = 3, cex = 0.5
)
```

